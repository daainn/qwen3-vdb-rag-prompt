{"cells":[{"cell_type":"code","execution_count":null,"metadata":{"scrolled":true,"id":"FjTi0FRxJF-G","outputId":"96482eb1-4849-4475-9c1c-f06a5c4b86ee"},"outputs":[{"name":"stdout","output_type":"stream","text":["Requirement already satisfied: torch==2.1.1 in /usr/local/lib/python3.10/dist-packages (2.1.1)\n","Requirement already satisfied: torchvision==0.16.1 in /usr/local/lib/python3.10/dist-packages (0.16.1)\n","Requirement already satisfied: torchaudio==2.1.1 in /usr/local/lib/python3.10/dist-packages (2.1.1)\n","Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from torch==2.1.1) (3.13.1)\n","Requirement already satisfied: typing-extensions in /usr/local/lib/python3.10/dist-packages (from torch==2.1.1) (4.8.0)\n","Requirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from torch==2.1.1) (1.12)\n","Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch==2.1.1) (3.2.1)\n","Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch==2.1.1) (3.1.2)\n","Requirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from torch==2.1.1) (2023.10.0)\n","Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.1.105 in /usr/local/lib/python3.10/dist-packages (from torch==2.1.1) (12.1.105)\n","Requirement already satisfied: nvidia-cuda-runtime-cu12==12.1.105 in /usr/local/lib/python3.10/dist-packages (from torch==2.1.1) (12.1.105)\n","Requirement already satisfied: nvidia-cuda-cupti-cu12==12.1.105 in /usr/local/lib/python3.10/dist-packages (from torch==2.1.1) (12.1.105)\n","Requirement already satisfied: nvidia-cudnn-cu12==8.9.2.26 in /usr/local/lib/python3.10/dist-packages (from torch==2.1.1) (8.9.2.26)\n","Requirement already satisfied: nvidia-cublas-cu12==12.1.3.1 in /usr/local/lib/python3.10/dist-packages (from torch==2.1.1) (12.1.3.1)\n","Requirement already satisfied: nvidia-cufft-cu12==11.0.2.54 in /usr/local/lib/python3.10/dist-packages (from torch==2.1.1) (11.0.2.54)\n","Requirement already satisfied: nvidia-curand-cu12==10.3.2.106 in /usr/local/lib/python3.10/dist-packages (from torch==2.1.1) (10.3.2.106)\n","Requirement already satisfied: nvidia-cusolver-cu12==11.4.5.107 in /usr/local/lib/python3.10/dist-packages (from torch==2.1.1) (11.4.5.107)\n","Requirement already satisfied: nvidia-cusparse-cu12==12.1.0.106 in /usr/local/lib/python3.10/dist-packages (from torch==2.1.1) (12.1.0.106)\n","Requirement already satisfied: nvidia-nccl-cu12==2.18.1 in /usr/local/lib/python3.10/dist-packages (from torch==2.1.1) (2.18.1)\n","Requirement already satisfied: nvidia-nvtx-cu12==12.1.105 in /usr/local/lib/python3.10/dist-packages (from torch==2.1.1) (12.1.105)\n","Requirement already satisfied: triton==2.1.0 in /usr/local/lib/python3.10/dist-packages (from torch==2.1.1) (2.1.0)\n","Requirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (from torchvision==0.16.1) (1.26.2)\n","Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from torchvision==0.16.1) (2.31.0)\n","Requirement already satisfied: pillow!=8.3.*,>=5.3.0 in /usr/local/lib/python3.10/dist-packages (from torchvision==0.16.1) (10.1.0)\n","Requirement already satisfied: nvidia-nvjitlink-cu12 in /usr/local/lib/python3.10/dist-packages (from nvidia-cusolver-cu12==11.4.5.107->torch==2.1.1) (12.3.101)\n","Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch==2.1.1) (2.1.3)\n","Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->torchvision==0.16.1) (3.3.2)\n","Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->torchvision==0.16.1) (3.6)\n","Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->torchvision==0.16.1) (2.1.0)\n","Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->torchvision==0.16.1) (2023.11.17)\n","Requirement already satisfied: mpmath>=0.19 in /usr/local/lib/python3.10/dist-packages (from sympy->torch==2.1.1) (1.3.0)\n","\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n","\u001b[0m\n","\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m23.3.1\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m25.1.1\u001b[0m\n","\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpython -m pip install --upgrade pip\u001b[0m\n","Collecting transformers\n","  Downloading transformers-4.51.3-py3-none-any.whl.metadata (38 kB)\n","Collecting accelerate\n","  Downloading accelerate-1.6.0-py3-none-any.whl.metadata (19 kB)\n","Collecting datasets\n","  Downloading datasets-3.6.0-py3-none-any.whl.metadata (19 kB)\n","Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from transformers) (3.13.1)\n","Collecting huggingface-hub<1.0,>=0.30.0 (from transformers)\n","  Downloading huggingface_hub-0.31.1-py3-none-any.whl.metadata (13 kB)\n","Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.10/dist-packages (from transformers) (1.26.2)\n","Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from transformers) (23.2)\n","Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/dist-packages (from transformers) (6.0.1)\n","Collecting regex!=2019.12.17 (from transformers)\n","  Downloading regex-2024.11.6-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (40 kB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m40.5/40.5 kB\u001b[0m \u001b[31m6.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hRequirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from transformers) (2.31.0)\n","Collecting tokenizers<0.22,>=0.21 (from transformers)\n","  Downloading tokenizers-0.21.1-cp39-abi3-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (6.8 kB)\n","Collecting safetensors>=0.4.3 (from transformers)\n","  Downloading safetensors-0.5.3-cp38-abi3-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (3.8 kB)\n","Collecting tqdm>=4.27 (from transformers)\n","  Downloading tqdm-4.67.1-py3-none-any.whl.metadata (57 kB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m57.7/57.7 kB\u001b[0m \u001b[31m4.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hRequirement already satisfied: psutil in /usr/local/lib/python3.10/dist-packages (from accelerate) (5.9.6)\n","Requirement already satisfied: torch>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from accelerate) (2.1.1)\n","Collecting pyarrow>=15.0.0 (from datasets)\n","  Downloading pyarrow-20.0.0-cp310-cp310-manylinux_2_28_x86_64.whl.metadata (3.3 kB)\n","Collecting dill<0.3.9,>=0.3.0 (from datasets)\n","  Downloading dill-0.3.8-py3-none-any.whl.metadata (10 kB)\n","Collecting pandas (from datasets)\n","  Downloading pandas-2.2.3-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (89 kB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m89.9/89.9 kB\u001b[0m \u001b[31m6.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hCollecting requests (from transformers)\n","  Downloading requests-2.32.3-py3-none-any.whl.metadata (4.6 kB)\n","Collecting xxhash (from datasets)\n","  Downloading xxhash-3.5.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (12 kB)\n","Collecting multiprocess<0.70.17 (from datasets)\n","  Downloading multiprocess-0.70.16-py310-none-any.whl.metadata (7.2 kB)\n","Requirement already satisfied: fsspec<=2025.3.0,>=2023.1.0 in /usr/local/lib/python3.10/dist-packages (from fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (2023.10.0)\n","Collecting aiohttp!=4.0.0a0,!=4.0.0a1 (from fsspec[http]<=2025.3.0,>=2023.1.0->datasets)\n","  Downloading aiohttp-3.11.18-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (7.7 kB)\n","Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub<1.0,>=0.30.0->transformers) (4.8.0)\n","Collecting hf-xet<2.0.0,>=1.1.0 (from huggingface-hub<1.0,>=0.30.0->transformers)\n","  Downloading hf_xet-1.1.0-cp37-abi3-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (494 bytes)\n","Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (3.3.2)\n","Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (3.6)\n","Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (2.1.0)\n","Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (2023.11.17)\n","Requirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from torch>=2.0.0->accelerate) (1.12)\n","Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch>=2.0.0->accelerate) (3.2.1)\n","Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch>=2.0.0->accelerate) (3.1.2)\n","Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.1.105 in /usr/local/lib/python3.10/dist-packages (from torch>=2.0.0->accelerate) (12.1.105)\n","Requirement already satisfied: nvidia-cuda-runtime-cu12==12.1.105 in /usr/local/lib/python3.10/dist-packages (from torch>=2.0.0->accelerate) (12.1.105)\n","Requirement already satisfied: nvidia-cuda-cupti-cu12==12.1.105 in /usr/local/lib/python3.10/dist-packages (from torch>=2.0.0->accelerate) (12.1.105)\n","Requirement already satisfied: nvidia-cudnn-cu12==8.9.2.26 in /usr/local/lib/python3.10/dist-packages (from torch>=2.0.0->accelerate) (8.9.2.26)\n","Requirement already satisfied: nvidia-cublas-cu12==12.1.3.1 in /usr/local/lib/python3.10/dist-packages (from torch>=2.0.0->accelerate) (12.1.3.1)\n","Requirement already satisfied: nvidia-cufft-cu12==11.0.2.54 in /usr/local/lib/python3.10/dist-packages (from torch>=2.0.0->accelerate) (11.0.2.54)\n","Requirement already satisfied: nvidia-curand-cu12==10.3.2.106 in /usr/local/lib/python3.10/dist-packages (from torch>=2.0.0->accelerate) (10.3.2.106)\n","Requirement already satisfied: nvidia-cusolver-cu12==11.4.5.107 in /usr/local/lib/python3.10/dist-packages (from torch>=2.0.0->accelerate) (11.4.5.107)\n","Requirement already satisfied: nvidia-cusparse-cu12==12.1.0.106 in /usr/local/lib/python3.10/dist-packages (from torch>=2.0.0->accelerate) (12.1.0.106)\n","Requirement already satisfied: nvidia-nccl-cu12==2.18.1 in /usr/local/lib/python3.10/dist-packages (from torch>=2.0.0->accelerate) (2.18.1)\n","Requirement already satisfied: nvidia-nvtx-cu12==12.1.105 in /usr/local/lib/python3.10/dist-packages (from torch>=2.0.0->accelerate) (12.1.105)\n","Requirement already satisfied: triton==2.1.0 in /usr/local/lib/python3.10/dist-packages (from torch>=2.0.0->accelerate) (2.1.0)\n","Requirement already satisfied: nvidia-nvjitlink-cu12 in /usr/local/lib/python3.10/dist-packages (from nvidia-cusolver-cu12==11.4.5.107->torch>=2.0.0->accelerate) (12.3.101)\n","Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.10/dist-packages (from pandas->datasets) (2.8.2)\n","Collecting pytz>=2020.1 (from pandas->datasets)\n","  Downloading pytz-2025.2-py2.py3-none-any.whl.metadata (22 kB)\n","Collecting tzdata>=2022.7 (from pandas->datasets)\n","  Downloading tzdata-2025.2-py2.py3-none-any.whl.metadata (1.4 kB)\n","Collecting aiohappyeyeballs>=2.3.0 (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets)\n","  Downloading aiohappyeyeballs-2.6.1-py3-none-any.whl.metadata (5.9 kB)\n","Collecting aiosignal>=1.1.2 (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets)\n","  Downloading aiosignal-1.3.2-py2.py3-none-any.whl.metadata (3.8 kB)\n","Collecting async-timeout<6.0,>=4.0 (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets)\n","  Downloading async_timeout-5.0.1-py3-none-any.whl.metadata (5.1 kB)\n","Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (23.1.0)\n","Collecting frozenlist>=1.1.1 (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets)\n","  Downloading frozenlist-1.6.0-cp310-cp310-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (16 kB)\n","Collecting multidict<7.0,>=4.5 (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets)\n","  Downloading multidict-6.4.3-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (5.3 kB)\n","Collecting propcache>=0.2.0 (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets)\n","  Downloading propcache-0.3.1-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (10 kB)\n","Collecting yarl<2.0,>=1.17.0 (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets)\n","  Downloading yarl-1.20.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (72 kB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m72.4/72.4 kB\u001b[0m \u001b[31m10.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hRequirement already satisfied: six>=1.5 in /usr/lib/python3/dist-packages (from python-dateutil>=2.8.2->pandas->datasets) (1.16.0)\n","Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch>=2.0.0->accelerate) (2.1.3)\n","Requirement already satisfied: mpmath>=0.19 in /usr/local/lib/python3.10/dist-packages (from sympy->torch>=2.0.0->accelerate) (1.3.0)\n","Downloading transformers-4.51.3-py3-none-any.whl (10.4 MB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m10.4/10.4 MB\u001b[0m \u001b[31m41.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m0:01\u001b[0m\n","\u001b[?25hDownloading accelerate-1.6.0-py3-none-any.whl (354 kB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m354.7/354.7 kB\u001b[0m \u001b[31m67.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hDownloading datasets-3.6.0-py3-none-any.whl (491 kB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m491.5/491.5 kB\u001b[0m \u001b[31m129.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hDownloading dill-0.3.8-py3-none-any.whl (116 kB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m116.3/116.3 kB\u001b[0m \u001b[31m59.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hDownloading huggingface_hub-0.31.1-py3-none-any.whl (484 kB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m484.3/484.3 kB\u001b[0m \u001b[31m57.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hDownloading multiprocess-0.70.16-py310-none-any.whl (134 kB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m134.8/134.8 kB\u001b[0m \u001b[31m28.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hDownloading pyarrow-20.0.0-cp310-cp310-manylinux_2_28_x86_64.whl (42.3 MB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m42.3/42.3 MB\u001b[0m \u001b[31m48.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n","\u001b[?25hDownloading regex-2024.11.6-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (781 kB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m781.7/781.7 kB\u001b[0m \u001b[31m144.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hDownloading requests-2.32.3-py3-none-any.whl (64 kB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m64.9/64.9 kB\u001b[0m \u001b[31m33.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hDownloading safetensors-0.5.3-cp38-abi3-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (471 kB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m471.6/471.6 kB\u001b[0m \u001b[31m126.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hDownloading tokenizers-0.21.1-cp39-abi3-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (3.0 MB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.0/3.0 MB\u001b[0m \u001b[31m85.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m\n","\u001b[?25hDownloading tqdm-4.67.1-py3-none-any.whl (78 kB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m78.5/78.5 kB\u001b[0m \u001b[31m41.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hDownloading pandas-2.2.3-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (13.1 MB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m13.1/13.1 MB\u001b[0m \u001b[31m53.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n","\u001b[?25hDownloading xxhash-3.5.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (194 kB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m194.1/194.1 kB\u001b[0m \u001b[31m70.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hDownloading aiohttp-3.11.18-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.6 MB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.6/1.6 MB\u001b[0m \u001b[31m6.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0mmm\n","\u001b[?25hDownloading hf_xet-1.1.0-cp37-abi3-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (53.6 MB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m53.6/53.6 MB\u001b[0m \u001b[31m41.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n","\u001b[?25hDownloading pytz-2025.2-py2.py3-none-any.whl (509 kB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m509.2/509.2 kB\u001b[0m \u001b[31m120.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hDownloading tzdata-2025.2-py2.py3-none-any.whl (347 kB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m347.8/347.8 kB\u001b[0m \u001b[31m111.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hDownloading aiohappyeyeballs-2.6.1-py3-none-any.whl (15 kB)\n","Downloading aiosignal-1.3.2-py2.py3-none-any.whl (7.6 kB)\n","Downloading async_timeout-5.0.1-py3-none-any.whl (6.2 kB)\n","Downloading frozenlist-1.6.0-cp310-cp310-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl (287 kB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m287.3/287.3 kB\u001b[0m \u001b[31m44.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hDownloading multidict-6.4.3-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (219 kB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m219.8/219.8 kB\u001b[0m \u001b[31m30.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hDownloading propcache-0.3.1-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (206 kB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m206.6/206.6 kB\u001b[0m \u001b[31m84.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hDownloading yarl-1.20.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (333 kB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m333.9/333.9 kB\u001b[0m \u001b[31m48.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hInstalling collected packages: pytz, xxhash, tzdata, tqdm, safetensors, requests, regex, pyarrow, propcache, multidict, hf-xet, frozenlist, dill, async-timeout, aiohappyeyeballs, yarl, pandas, multiprocess, huggingface-hub, aiosignal, tokenizers, aiohttp, transformers, accelerate, datasets\n","  Attempting uninstall: requests\n","    Found existing installation: requests 2.31.0\n","    Uninstalling requests-2.31.0:\n","      Successfully uninstalled requests-2.31.0\n","Successfully installed accelerate-1.6.0 aiohappyeyeballs-2.6.1 aiohttp-3.11.18 aiosignal-1.3.2 async-timeout-5.0.1 datasets-3.6.0 dill-0.3.8 frozenlist-1.6.0 hf-xet-1.1.0 huggingface-hub-0.31.1 multidict-6.4.3 multiprocess-0.70.16 pandas-2.2.3 propcache-0.3.1 pyarrow-20.0.0 pytz-2025.2 regex-2024.11.6 requests-2.32.3 safetensors-0.5.3 tokenizers-0.21.1 tqdm-4.67.1 transformers-4.51.3 tzdata-2025.2 xxhash-3.5.0 yarl-1.20.0\n","\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n","\u001b[0m\n","\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m23.3.1\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m25.1.1\u001b[0m\n","\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpython -m pip install --upgrade pip\u001b[0m\n","Collecting peft\n","  Downloading peft-0.15.2-py3-none-any.whl.metadata (13 kB)\n","Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.10/dist-packages (from peft) (1.26.2)\n","Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from peft) (23.2)\n","Requirement already satisfied: psutil in /usr/local/lib/python3.10/dist-packages (from peft) (5.9.6)\n","Requirement already satisfied: pyyaml in /usr/local/lib/python3.10/dist-packages (from peft) (6.0.1)\n","Requirement already satisfied: torch>=1.13.0 in /usr/local/lib/python3.10/dist-packages (from peft) (2.1.1)\n","Requirement already satisfied: transformers in /usr/local/lib/python3.10/dist-packages (from peft) (4.51.3)\n","Requirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (from peft) (4.67.1)\n","Requirement already satisfied: accelerate>=0.21.0 in /usr/local/lib/python3.10/dist-packages (from peft) (1.6.0)\n","Requirement already satisfied: safetensors in /usr/local/lib/python3.10/dist-packages (from peft) (0.5.3)\n","Requirement already satisfied: huggingface_hub>=0.25.0 in /usr/local/lib/python3.10/dist-packages (from peft) (0.31.1)\n","Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from huggingface_hub>=0.25.0->peft) (3.13.1)\n","Requirement already satisfied: fsspec>=2023.5.0 in /usr/local/lib/python3.10/dist-packages (from huggingface_hub>=0.25.0->peft) (2023.10.0)\n","Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from huggingface_hub>=0.25.0->peft) (2.32.3)\n","Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.10/dist-packages (from huggingface_hub>=0.25.0->peft) (4.8.0)\n","Requirement already satisfied: hf-xet<2.0.0,>=1.1.0 in /usr/local/lib/python3.10/dist-packages (from huggingface_hub>=0.25.0->peft) (1.1.0)\n","Requirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from torch>=1.13.0->peft) (1.12)\n","Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch>=1.13.0->peft) (3.2.1)\n","Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch>=1.13.0->peft) (3.1.2)\n","Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.1.105 in /usr/local/lib/python3.10/dist-packages (from torch>=1.13.0->peft) (12.1.105)\n","Requirement already satisfied: nvidia-cuda-runtime-cu12==12.1.105 in /usr/local/lib/python3.10/dist-packages (from torch>=1.13.0->peft) (12.1.105)\n","Requirement already satisfied: nvidia-cuda-cupti-cu12==12.1.105 in /usr/local/lib/python3.10/dist-packages (from torch>=1.13.0->peft) (12.1.105)\n","Requirement already satisfied: nvidia-cudnn-cu12==8.9.2.26 in /usr/local/lib/python3.10/dist-packages (from torch>=1.13.0->peft) (8.9.2.26)\n","Requirement already satisfied: nvidia-cublas-cu12==12.1.3.1 in /usr/local/lib/python3.10/dist-packages (from torch>=1.13.0->peft) (12.1.3.1)\n","Requirement already satisfied: nvidia-cufft-cu12==11.0.2.54 in /usr/local/lib/python3.10/dist-packages (from torch>=1.13.0->peft) (11.0.2.54)\n","Requirement already satisfied: nvidia-curand-cu12==10.3.2.106 in /usr/local/lib/python3.10/dist-packages (from torch>=1.13.0->peft) (10.3.2.106)\n","Requirement already satisfied: nvidia-cusolver-cu12==11.4.5.107 in /usr/local/lib/python3.10/dist-packages (from torch>=1.13.0->peft) (11.4.5.107)\n","Requirement already satisfied: nvidia-cusparse-cu12==12.1.0.106 in /usr/local/lib/python3.10/dist-packages (from torch>=1.13.0->peft) (12.1.0.106)\n","Requirement already satisfied: nvidia-nccl-cu12==2.18.1 in /usr/local/lib/python3.10/dist-packages (from torch>=1.13.0->peft) (2.18.1)\n","Requirement already satisfied: nvidia-nvtx-cu12==12.1.105 in /usr/local/lib/python3.10/dist-packages (from torch>=1.13.0->peft) (12.1.105)\n","Requirement already satisfied: triton==2.1.0 in /usr/local/lib/python3.10/dist-packages (from torch>=1.13.0->peft) (2.1.0)\n","Requirement already satisfied: nvidia-nvjitlink-cu12 in /usr/local/lib/python3.10/dist-packages (from nvidia-cusolver-cu12==11.4.5.107->torch>=1.13.0->peft) (12.3.101)\n","Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.10/dist-packages (from transformers->peft) (2024.11.6)\n","Requirement already satisfied: tokenizers<0.22,>=0.21 in /usr/local/lib/python3.10/dist-packages (from transformers->peft) (0.21.1)\n","Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch>=1.13.0->peft) (2.1.3)\n","Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface_hub>=0.25.0->peft) (3.3.2)\n","Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface_hub>=0.25.0->peft) (3.6)\n","Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface_hub>=0.25.0->peft) (2.1.0)\n","Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface_hub>=0.25.0->peft) (2023.11.17)\n","Requirement already satisfied: mpmath>=0.19 in /usr/local/lib/python3.10/dist-packages (from sympy->torch>=1.13.0->peft) (1.3.0)\n","Downloading peft-0.15.2-py3-none-any.whl (411 kB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m411.1/411.1 kB\u001b[0m \u001b[31m3.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n","\u001b[?25hInstalling collected packages: peft\n","Successfully installed peft-0.15.2\n","\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n","\u001b[0m\n","\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m23.3.1\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m25.1.1\u001b[0m\n","\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpython -m pip install --upgrade pip\u001b[0m\n"]}],"source":["!pip install torch==2.1.1 torchvision==0.16.1 torchaudio==2.1.1\n","!pip install transformers accelerate datasets\n","!pip install peft"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"nEmjfwMr9zst","colab":{"referenced_widgets":["673d8226d8ea4ec8a88f33d38dbc5ce2","880ddc6d16af49bd8aabca700452a598"]},"outputId":"beb6bef6-03bf-444d-b18d-8c82b55b4ab2"},"outputs":[{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"673d8226d8ea4ec8a88f33d38dbc5ce2","version_major":2,"version_minor":0},"text/plain":["Loading checkpoint shards:   0%|          | 0/5 [00:00<?, ?it/s]"]},"metadata":{},"output_type":"display_data"},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"880ddc6d16af49bd8aabca700452a598","version_major":2,"version_minor":0},"text/plain":["Map:   0%|          | 0/2452 [00:00<?, ? examples/s]"]},"metadata":{},"output_type":"display_data"},{"name":"stderr","output_type":"stream","text":["No label_names provided for model class `PeftModelForCausalLM`. Since `PeftModel` hides base models input arguments, if label_names is not given, label_names can't be set automatically within `Trainer`. Note that empty label_names list will be used instead.\n"]},{"data":{"text/html":["\n","    <div>\n","      \n","      <progress value='918' max='918' style='width:300px; height:20px; vertical-align: middle;'></progress>\n","      [918/918 43:55, Epoch 2/3]\n","    </div>\n","    <table border=\"1\" class=\"dataframe\">\n","  <thead>\n"," <tr style=\"text-align: left;\">\n","      <th>Step</th>\n","      <th>Training Loss</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <td>10</td>\n","      <td>2.702600</td>\n","    </tr>\n","    <tr>\n","      <td>20</td>\n","      <td>2.608400</td>\n","    </tr>\n","    <tr>\n","      <td>30</td>\n","      <td>2.532700</td>\n","    </tr>\n","    <tr>\n","      <td>40</td>\n","      <td>2.450800</td>\n","    </tr>\n","    <tr>\n","      <td>50</td>\n","      <td>2.449800</td>\n","    </tr>\n","    <tr>\n","      <td>60</td>\n","      <td>2.424200</td>\n","    </tr>\n","    <tr>\n","      <td>70</td>\n","      <td>2.340700</td>\n","    </tr>\n","    <tr>\n","      <td>80</td>\n","      <td>2.294200</td>\n","    </tr>\n","    <tr>\n","      <td>90</td>\n","      <td>2.285700</td>\n","    </tr>\n","    <tr>\n","      <td>100</td>\n","      <td>2.263800</td>\n","    </tr>\n","    <tr>\n","      <td>110</td>\n","      <td>2.202200</td>\n","    </tr>\n","    <tr>\n","      <td>120</td>\n","      <td>2.186800</td>\n","    </tr>\n","    <tr>\n","      <td>130</td>\n","      <td>2.191600</td>\n","    </tr>\n","    <tr>\n","      <td>140</td>\n","      <td>2.160000</td>\n","    </tr>\n","    <tr>\n","      <td>150</td>\n","      <td>2.148700</td>\n","    </tr>\n","    <tr>\n","      <td>160</td>\n","      <td>2.178000</td>\n","    </tr>\n","    <tr>\n","      <td>170</td>\n","      <td>2.108600</td>\n","    </tr>\n","    <tr>\n","      <td>180</td>\n","      <td>2.080400</td>\n","    </tr>\n","    <tr>\n","      <td>190</td>\n","      <td>2.087000</td>\n","    </tr>\n","    <tr>\n","      <td>200</td>\n","      <td>2.072400</td>\n","    </tr>\n","    <tr>\n","      <td>210</td>\n","      <td>2.046600</td>\n","    </tr>\n","    <tr>\n","      <td>220</td>\n","      <td>2.044000</td>\n","    </tr>\n","    <tr>\n","      <td>230</td>\n","      <td>2.046700</td>\n","    </tr>\n","    <tr>\n","      <td>240</td>\n","      <td>2.042800</td>\n","    </tr>\n","    <tr>\n","      <td>250</td>\n","      <td>2.032000</td>\n","    </tr>\n","    <tr>\n","      <td>260</td>\n","      <td>2.029700</td>\n","    </tr>\n","    <tr>\n","      <td>270</td>\n","      <td>1.986200</td>\n","    </tr>\n","    <tr>\n","      <td>280</td>\n","      <td>1.995800</td>\n","    </tr>\n","    <tr>\n","      <td>290</td>\n","      <td>1.995800</td>\n","    </tr>\n","    <tr>\n","      <td>300</td>\n","      <td>2.015800</td>\n","    </tr>\n","    <tr>\n","      <td>310</td>\n","      <td>1.968100</td>\n","    </tr>\n","    <tr>\n","      <td>320</td>\n","      <td>1.985100</td>\n","    </tr>\n","    <tr>\n","      <td>330</td>\n","      <td>1.953200</td>\n","    </tr>\n","    <tr>\n","      <td>340</td>\n","      <td>1.977300</td>\n","    </tr>\n","    <tr>\n","      <td>350</td>\n","      <td>1.970300</td>\n","    </tr>\n","    <tr>\n","      <td>360</td>\n","      <td>1.959800</td>\n","    </tr>\n","    <tr>\n","      <td>370</td>\n","      <td>1.956100</td>\n","    </tr>\n","    <tr>\n","      <td>380</td>\n","      <td>1.939500</td>\n","    </tr>\n","    <tr>\n","      <td>390</td>\n","      <td>1.940500</td>\n","    </tr>\n","    <tr>\n","      <td>400</td>\n","      <td>1.914900</td>\n","    </tr>\n","    <tr>\n","      <td>410</td>\n","      <td>1.940300</td>\n","    </tr>\n","    <tr>\n","      <td>420</td>\n","      <td>1.916400</td>\n","    </tr>\n","    <tr>\n","      <td>430</td>\n","      <td>1.934300</td>\n","    </tr>\n","    <tr>\n","      <td>440</td>\n","      <td>1.928700</td>\n","    </tr>\n","    <tr>\n","      <td>450</td>\n","      <td>1.918400</td>\n","    </tr>\n","    <tr>\n","      <td>460</td>\n","      <td>1.926000</td>\n","    </tr>\n","    <tr>\n","      <td>470</td>\n","      <td>1.897100</td>\n","    </tr>\n","    <tr>\n","      <td>480</td>\n","      <td>1.893200</td>\n","    </tr>\n","    <tr>\n","      <td>490</td>\n","      <td>1.854400</td>\n","    </tr>\n","    <tr>\n","      <td>500</td>\n","      <td>1.873100</td>\n","    </tr>\n","    <tr>\n","      <td>510</td>\n","      <td>1.847100</td>\n","    </tr>\n","    <tr>\n","      <td>520</td>\n","      <td>1.919400</td>\n","    </tr>\n","    <tr>\n","      <td>530</td>\n","      <td>1.898400</td>\n","    </tr>\n","    <tr>\n","      <td>540</td>\n","      <td>1.862200</td>\n","    </tr>\n","    <tr>\n","      <td>550</td>\n","      <td>1.875800</td>\n","    </tr>\n","    <tr>\n","      <td>560</td>\n","      <td>1.878100</td>\n","    </tr>\n","    <tr>\n","      <td>570</td>\n","      <td>1.843700</td>\n","    </tr>\n","    <tr>\n","      <td>580</td>\n","      <td>1.888900</td>\n","    </tr>\n","    <tr>\n","      <td>590</td>\n","      <td>1.858500</td>\n","    </tr>\n","    <tr>\n","      <td>600</td>\n","      <td>1.821200</td>\n","    </tr>\n","    <tr>\n","      <td>610</td>\n","      <td>1.848300</td>\n","    </tr>\n","    <tr>\n","      <td>620</td>\n","      <td>1.839000</td>\n","    </tr>\n","    <tr>\n","      <td>630</td>\n","      <td>1.852500</td>\n","    </tr>\n","    <tr>\n","      <td>640</td>\n","      <td>1.853800</td>\n","    </tr>\n","    <tr>\n","      <td>650</td>\n","      <td>1.814500</td>\n","    </tr>\n","    <tr>\n","      <td>660</td>\n","      <td>1.773600</td>\n","    </tr>\n","    <tr>\n","      <td>670</td>\n","      <td>1.824100</td>\n","    </tr>\n","    <tr>\n","      <td>680</td>\n","      <td>1.855600</td>\n","    </tr>\n","    <tr>\n","      <td>690</td>\n","      <td>1.880300</td>\n","    </tr>\n","    <tr>\n","      <td>700</td>\n","      <td>1.809700</td>\n","    </tr>\n","    <tr>\n","      <td>710</td>\n","      <td>1.810900</td>\n","    </tr>\n","    <tr>\n","      <td>720</td>\n","      <td>1.860600</td>\n","    </tr>\n","    <tr>\n","      <td>730</td>\n","      <td>1.775300</td>\n","    </tr>\n","    <tr>\n","      <td>740</td>\n","      <td>1.810800</td>\n","    </tr>\n","    <tr>\n","      <td>750</td>\n","      <td>1.826900</td>\n","    </tr>\n","    <tr>\n","      <td>760</td>\n","      <td>1.788200</td>\n","    </tr>\n","    <tr>\n","      <td>770</td>\n","      <td>1.858200</td>\n","    </tr>\n","    <tr>\n","      <td>780</td>\n","      <td>1.849700</td>\n","    </tr>\n","    <tr>\n","      <td>790</td>\n","      <td>1.800800</td>\n","    </tr>\n","    <tr>\n","      <td>800</td>\n","      <td>1.869500</td>\n","    </tr>\n","    <tr>\n","      <td>810</td>\n","      <td>1.834200</td>\n","    </tr>\n","    <tr>\n","      <td>820</td>\n","      <td>1.838200</td>\n","    </tr>\n","    <tr>\n","      <td>830</td>\n","      <td>1.810000</td>\n","    </tr>\n","    <tr>\n","      <td>840</td>\n","      <td>1.828300</td>\n","    </tr>\n","    <tr>\n","      <td>850</td>\n","      <td>1.801600</td>\n","    </tr>\n","    <tr>\n","      <td>860</td>\n","      <td>1.797800</td>\n","    </tr>\n","    <tr>\n","      <td>870</td>\n","      <td>1.713500</td>\n","    </tr>\n","    <tr>\n","      <td>880</td>\n","      <td>1.793100</td>\n","    </tr>\n","    <tr>\n","      <td>890</td>\n","      <td>1.783700</td>\n","    </tr>\n","    <tr>\n","      <td>900</td>\n","      <td>1.843000</td>\n","    </tr>\n","    <tr>\n","      <td>910</td>\n","      <td>1.801500</td>\n","    </tr>\n","  </tbody>\n","</table><p>"],"text/plain":["<IPython.core.display.HTML object>"]},"metadata":{},"output_type":"display_data"},{"data":{"text/plain":["TrainOutput(global_step=918, training_loss=1.9744175524493448, metrics={'train_runtime': 2638.9497, 'train_samples_per_second': 2.787, 'train_steps_per_second': 0.348, 'total_flos': 3.137344340725801e+17, 'train_loss': 1.9744175524493448, 'epoch': 2.99184339314845})"]},"execution_count":5,"metadata":{},"output_type":"execute_result"}],"source":["from datasets import Dataset\n","from transformers import (\n","    AutoTokenizer,\n","    AutoModelForCausalLM,\n","    TrainingArguments,\n","    Trainer,\n","    DataCollatorForLanguageModeling\n",")\n","from peft import LoraConfig, get_peft_model\n","import torch\n","import json\n","import random\n","\n","# === Step 1: Load ChatML Format Dataset ===\n","with open(\"6_wayo_qna_data.json\", \"r\", encoding=\"utf-8\") as f:\n","    raw_data = json.load(f)\n","\n","dataset = Dataset.from_list(raw_data)\n","\n","# === Step 2: Load Tokenizer & Base Model ===\n","model_name = \"Qwen/Qwen3-8B\"\n","tokenizer = AutoTokenizer.from_pretrained(model_name, trust_remote_code=True)\n","model = AutoModelForCausalLM.from_pretrained(\n","    model_name,\n","    trust_remote_code=True,\n","    torch_dtype=torch.float16\n",")\n","\n","# === Step 3: Apply LoRA Adapter ===\n","lora_config = LoraConfig(\n","    r=8,\n","    lora_alpha=16,\n","    lora_dropout=0.03,\n","    bias=\"none\",\n","    task_type=\"CAUSAL_LM\",\n","    target_modules=[\n","        \"q_proj\", \"k_proj\", \"v_proj\", \"o_proj\",\n","        \"gate_proj\", \"up_proj\", \"down_proj\"\n","    ]\n",")\n","model = get_peft_model(model, lora_config)\n","model = model.to(\"cuda\")\n","\n","# === Step 4: Tokenize Chat Format ===\n","def tokenize(example):\n","    if random.random() < 0.7:\n","        user_input = example.get(\"증상과 행동\", \"\").strip()\n","    else:\n","        user_input = f\"\"\"📍 증상 및 행동: {example.get(\"증상과 행동\", \"\")}\\n🕒 시작 시점: {example.get(\"시작된 시점\", \"\")}\\n👤 보호자 반응: {example.get(\"보호자님 반응\", \"\")}\"\"\".strip()\n","\n","    assistant_response = f\"\"\"🔍 원인 분석:\\n{example.get(\"원인 분석\", \"\")}\\n\\n💡 솔루션 제안:\\n{example.get(\"솔루션 제안\", \"\")}\"\"\".strip()\n","\n","    messages = [\n","        {\"role\": \"user\", \"content\": user_input},\n","        {\"role\": \"assistant\", \"content\": assistant_response}\n","    ]\n","\n","    input_ids = tokenizer.apply_chat_template(\n","        messages,\n","        tokenize=True,\n","        truncation=True,\n","        max_length=2048\n","    )\n","\n","    return {\n","        \"input_ids\": input_ids,\n","        \"labels\": input_ids.copy()\n","    }\n","\n","tokenized_dataset = dataset.map(tokenize)\n","\n","# === Step 5: Training Arguments ===\n","training_args = TrainingArguments(\n","    output_dir=\"./qwen3-lora-output\",\n","    per_device_train_batch_size=1,\n","    gradient_accumulation_steps=8,\n","    num_train_epochs=3,\n","    learning_rate=1.5e-5,\n","    logging_steps=10,\n","    save_steps=100,\n","    save_total_limit=2,\n","    fp16=True,\n","    report_to=\"none\"\n",")\n","\n","# === Step 6: Trainer ===\n","trainer = Trainer(\n","    model=model,\n","    args=training_args,\n","    train_dataset=tokenized_dataset,\n","    data_collator=DataCollatorForLanguageModeling(tokenizer, mlm=False)\n",")\n","\n","# === Step 7: Train ===\n","trainer.train()\n"]},{"cell_type":"markdown","metadata":{"id":"l4SddIM7JF-M"},"source":["### qwen 모델 모듈 직접 확인하는 코드"]},{"cell_type":"code","execution_count":null,"metadata":{"scrolled":true,"id":"4tzBdlCLJF-O","outputId":"84d7a37b-74cd-45df-81b4-5237246feedc"},"outputs":[{"name":"stdout","output_type":"stream","text":["\n","model\n","model.embed_tokens\n","model.layers\n","model.layers.0\n","model.layers.0.self_attn\n","model.layers.0.self_attn.q_proj\n","model.layers.0.self_attn.k_proj\n","model.layers.0.self_attn.v_proj\n","model.layers.0.self_attn.o_proj\n","model.layers.0.self_attn.q_norm\n","model.layers.0.self_attn.k_norm\n","model.layers.0.mlp\n","model.layers.0.mlp.gate_proj\n","model.layers.0.mlp.up_proj\n","model.layers.0.mlp.down_proj\n","model.layers.0.mlp.act_fn\n","model.layers.0.input_layernorm\n","model.layers.0.post_attention_layernorm\n","model.layers.1\n","model.layers.1.self_attn\n","model.layers.1.self_attn.q_proj\n","model.layers.1.self_attn.k_proj\n","model.layers.1.self_attn.v_proj\n","model.layers.1.self_attn.o_proj\n","model.layers.1.self_attn.q_norm\n","model.layers.1.self_attn.k_norm\n","model.layers.1.mlp\n","model.layers.1.mlp.gate_proj\n","model.layers.1.mlp.up_proj\n","model.layers.1.mlp.down_proj\n","model.layers.1.mlp.act_fn\n","model.layers.1.input_layernorm\n","model.layers.1.post_attention_layernorm\n","model.layers.2\n","model.layers.2.self_attn\n","model.layers.2.self_attn.q_proj\n","model.layers.2.self_attn.k_proj\n","model.layers.2.self_attn.v_proj\n","model.layers.2.self_attn.o_proj\n","model.layers.2.self_attn.q_norm\n","model.layers.2.self_attn.k_norm\n","model.layers.2.mlp\n","model.layers.2.mlp.gate_proj\n","model.layers.2.mlp.up_proj\n","model.layers.2.mlp.down_proj\n","model.layers.2.mlp.act_fn\n","model.layers.2.input_layernorm\n","model.layers.2.post_attention_layernorm\n","model.layers.3\n","model.layers.3.self_attn\n","model.layers.3.self_attn.q_proj\n","model.layers.3.self_attn.k_proj\n","model.layers.3.self_attn.v_proj\n","model.layers.3.self_attn.o_proj\n","model.layers.3.self_attn.q_norm\n","model.layers.3.self_attn.k_norm\n","model.layers.3.mlp\n","model.layers.3.mlp.gate_proj\n","model.layers.3.mlp.up_proj\n","model.layers.3.mlp.down_proj\n","model.layers.3.mlp.act_fn\n","model.layers.3.input_layernorm\n","model.layers.3.post_attention_layernorm\n","model.layers.4\n","model.layers.4.self_attn\n","model.layers.4.self_attn.q_proj\n","model.layers.4.self_attn.k_proj\n","model.layers.4.self_attn.v_proj\n","model.layers.4.self_attn.o_proj\n","model.layers.4.self_attn.q_norm\n","model.layers.4.self_attn.k_norm\n","model.layers.4.mlp\n","model.layers.4.mlp.gate_proj\n","model.layers.4.mlp.up_proj\n","model.layers.4.mlp.down_proj\n","model.layers.4.mlp.act_fn\n","model.layers.4.input_layernorm\n","model.layers.4.post_attention_layernorm\n","model.layers.5\n","model.layers.5.self_attn\n","model.layers.5.self_attn.q_proj\n","model.layers.5.self_attn.k_proj\n","model.layers.5.self_attn.v_proj\n","model.layers.5.self_attn.o_proj\n","model.layers.5.self_attn.q_norm\n","model.layers.5.self_attn.k_norm\n","model.layers.5.mlp\n","model.layers.5.mlp.gate_proj\n","model.layers.5.mlp.up_proj\n","model.layers.5.mlp.down_proj\n","model.layers.5.mlp.act_fn\n","model.layers.5.input_layernorm\n","model.layers.5.post_attention_layernorm\n","model.layers.6\n","model.layers.6.self_attn\n","model.layers.6.self_attn.q_proj\n","model.layers.6.self_attn.k_proj\n","model.layers.6.self_attn.v_proj\n","model.layers.6.self_attn.o_proj\n","model.layers.6.self_attn.q_norm\n","model.layers.6.self_attn.k_norm\n","model.layers.6.mlp\n","model.layers.6.mlp.gate_proj\n","model.layers.6.mlp.up_proj\n","model.layers.6.mlp.down_proj\n","model.layers.6.mlp.act_fn\n","model.layers.6.input_layernorm\n","model.layers.6.post_attention_layernorm\n","model.layers.7\n","model.layers.7.self_attn\n","model.layers.7.self_attn.q_proj\n","model.layers.7.self_attn.k_proj\n","model.layers.7.self_attn.v_proj\n","model.layers.7.self_attn.o_proj\n","model.layers.7.self_attn.q_norm\n","model.layers.7.self_attn.k_norm\n","model.layers.7.mlp\n","model.layers.7.mlp.gate_proj\n","model.layers.7.mlp.up_proj\n","model.layers.7.mlp.down_proj\n","model.layers.7.mlp.act_fn\n","model.layers.7.input_layernorm\n","model.layers.7.post_attention_layernorm\n","model.layers.8\n","model.layers.8.self_attn\n","model.layers.8.self_attn.q_proj\n","model.layers.8.self_attn.k_proj\n","model.layers.8.self_attn.v_proj\n","model.layers.8.self_attn.o_proj\n","model.layers.8.self_attn.q_norm\n","model.layers.8.self_attn.k_norm\n","model.layers.8.mlp\n","model.layers.8.mlp.gate_proj\n","model.layers.8.mlp.up_proj\n","model.layers.8.mlp.down_proj\n","model.layers.8.mlp.act_fn\n","model.layers.8.input_layernorm\n","model.layers.8.post_attention_layernorm\n","model.layers.9\n","model.layers.9.self_attn\n","model.layers.9.self_attn.q_proj\n","model.layers.9.self_attn.k_proj\n","model.layers.9.self_attn.v_proj\n","model.layers.9.self_attn.o_proj\n","model.layers.9.self_attn.q_norm\n","model.layers.9.self_attn.k_norm\n","model.layers.9.mlp\n","model.layers.9.mlp.gate_proj\n","model.layers.9.mlp.up_proj\n","model.layers.9.mlp.down_proj\n","model.layers.9.mlp.act_fn\n","model.layers.9.input_layernorm\n","model.layers.9.post_attention_layernorm\n","model.layers.10\n","model.layers.10.self_attn\n","model.layers.10.self_attn.q_proj\n","model.layers.10.self_attn.k_proj\n","model.layers.10.self_attn.v_proj\n","model.layers.10.self_attn.o_proj\n","model.layers.10.self_attn.q_norm\n","model.layers.10.self_attn.k_norm\n","model.layers.10.mlp\n","model.layers.10.mlp.gate_proj\n","model.layers.10.mlp.up_proj\n","model.layers.10.mlp.down_proj\n","model.layers.10.mlp.act_fn\n","model.layers.10.input_layernorm\n","model.layers.10.post_attention_layernorm\n","model.layers.11\n","model.layers.11.self_attn\n","model.layers.11.self_attn.q_proj\n","model.layers.11.self_attn.k_proj\n","model.layers.11.self_attn.v_proj\n","model.layers.11.self_attn.o_proj\n","model.layers.11.self_attn.q_norm\n","model.layers.11.self_attn.k_norm\n","model.layers.11.mlp\n","model.layers.11.mlp.gate_proj\n","model.layers.11.mlp.up_proj\n","model.layers.11.mlp.down_proj\n","model.layers.11.mlp.act_fn\n","model.layers.11.input_layernorm\n","model.layers.11.post_attention_layernorm\n","model.layers.12\n","model.layers.12.self_attn\n","model.layers.12.self_attn.q_proj\n","model.layers.12.self_attn.k_proj\n","model.layers.12.self_attn.v_proj\n","model.layers.12.self_attn.o_proj\n","model.layers.12.self_attn.q_norm\n","model.layers.12.self_attn.k_norm\n","model.layers.12.mlp\n","model.layers.12.mlp.gate_proj\n","model.layers.12.mlp.up_proj\n","model.layers.12.mlp.down_proj\n","model.layers.12.mlp.act_fn\n","model.layers.12.input_layernorm\n","model.layers.12.post_attention_layernorm\n","model.layers.13\n","model.layers.13.self_attn\n","model.layers.13.self_attn.q_proj\n","model.layers.13.self_attn.k_proj\n","model.layers.13.self_attn.v_proj\n","model.layers.13.self_attn.o_proj\n","model.layers.13.self_attn.q_norm\n","model.layers.13.self_attn.k_norm\n","model.layers.13.mlp\n","model.layers.13.mlp.gate_proj\n","model.layers.13.mlp.up_proj\n","model.layers.13.mlp.down_proj\n","model.layers.13.mlp.act_fn\n","model.layers.13.input_layernorm\n","model.layers.13.post_attention_layernorm\n","model.layers.14\n","model.layers.14.self_attn\n","model.layers.14.self_attn.q_proj\n","model.layers.14.self_attn.k_proj\n","model.layers.14.self_attn.v_proj\n","model.layers.14.self_attn.o_proj\n","model.layers.14.self_attn.q_norm\n","model.layers.14.self_attn.k_norm\n","model.layers.14.mlp\n","model.layers.14.mlp.gate_proj\n","model.layers.14.mlp.up_proj\n","model.layers.14.mlp.down_proj\n","model.layers.14.mlp.act_fn\n","model.layers.14.input_layernorm\n","model.layers.14.post_attention_layernorm\n","model.layers.15\n","model.layers.15.self_attn\n","model.layers.15.self_attn.q_proj\n","model.layers.15.self_attn.k_proj\n","model.layers.15.self_attn.v_proj\n","model.layers.15.self_attn.o_proj\n","model.layers.15.self_attn.q_norm\n","model.layers.15.self_attn.k_norm\n","model.layers.15.mlp\n","model.layers.15.mlp.gate_proj\n","model.layers.15.mlp.up_proj\n","model.layers.15.mlp.down_proj\n","model.layers.15.mlp.act_fn\n","model.layers.15.input_layernorm\n","model.layers.15.post_attention_layernorm\n","model.layers.16\n","model.layers.16.self_attn\n","model.layers.16.self_attn.q_proj\n","model.layers.16.self_attn.k_proj\n","model.layers.16.self_attn.v_proj\n","model.layers.16.self_attn.o_proj\n","model.layers.16.self_attn.q_norm\n","model.layers.16.self_attn.k_norm\n","model.layers.16.mlp\n","model.layers.16.mlp.gate_proj\n","model.layers.16.mlp.up_proj\n","model.layers.16.mlp.down_proj\n","model.layers.16.mlp.act_fn\n","model.layers.16.input_layernorm\n","model.layers.16.post_attention_layernorm\n","model.layers.17\n","model.layers.17.self_attn\n","model.layers.17.self_attn.q_proj\n","model.layers.17.self_attn.k_proj\n","model.layers.17.self_attn.v_proj\n","model.layers.17.self_attn.o_proj\n","model.layers.17.self_attn.q_norm\n","model.layers.17.self_attn.k_norm\n","model.layers.17.mlp\n","model.layers.17.mlp.gate_proj\n","model.layers.17.mlp.up_proj\n","model.layers.17.mlp.down_proj\n","model.layers.17.mlp.act_fn\n","model.layers.17.input_layernorm\n","model.layers.17.post_attention_layernorm\n","model.layers.18\n","model.layers.18.self_attn\n","model.layers.18.self_attn.q_proj\n","model.layers.18.self_attn.k_proj\n","model.layers.18.self_attn.v_proj\n","model.layers.18.self_attn.o_proj\n","model.layers.18.self_attn.q_norm\n","model.layers.18.self_attn.k_norm\n","model.layers.18.mlp\n","model.layers.18.mlp.gate_proj\n","model.layers.18.mlp.up_proj\n","model.layers.18.mlp.down_proj\n","model.layers.18.mlp.act_fn\n","model.layers.18.input_layernorm\n","model.layers.18.post_attention_layernorm\n","model.layers.19\n","model.layers.19.self_attn\n","model.layers.19.self_attn.q_proj\n","model.layers.19.self_attn.k_proj\n","model.layers.19.self_attn.v_proj\n","model.layers.19.self_attn.o_proj\n","model.layers.19.self_attn.q_norm\n","model.layers.19.self_attn.k_norm\n","model.layers.19.mlp\n","model.layers.19.mlp.gate_proj\n","model.layers.19.mlp.up_proj\n","model.layers.19.mlp.down_proj\n","model.layers.19.mlp.act_fn\n","model.layers.19.input_layernorm\n","model.layers.19.post_attention_layernorm\n","model.layers.20\n","model.layers.20.self_attn\n","model.layers.20.self_attn.q_proj\n","model.layers.20.self_attn.k_proj\n","model.layers.20.self_attn.v_proj\n","model.layers.20.self_attn.o_proj\n","model.layers.20.self_attn.q_norm\n","model.layers.20.self_attn.k_norm\n","model.layers.20.mlp\n","model.layers.20.mlp.gate_proj\n","model.layers.20.mlp.up_proj\n","model.layers.20.mlp.down_proj\n","model.layers.20.mlp.act_fn\n","model.layers.20.input_layernorm\n","model.layers.20.post_attention_layernorm\n","model.layers.21\n","model.layers.21.self_attn\n","model.layers.21.self_attn.q_proj\n","model.layers.21.self_attn.k_proj\n","model.layers.21.self_attn.v_proj\n","model.layers.21.self_attn.o_proj\n","model.layers.21.self_attn.q_norm\n","model.layers.21.self_attn.k_norm\n","model.layers.21.mlp\n","model.layers.21.mlp.gate_proj\n","model.layers.21.mlp.up_proj\n","model.layers.21.mlp.down_proj\n","model.layers.21.mlp.act_fn\n","model.layers.21.input_layernorm\n","model.layers.21.post_attention_layernorm\n","model.layers.22\n","model.layers.22.self_attn\n","model.layers.22.self_attn.q_proj\n","model.layers.22.self_attn.k_proj\n","model.layers.22.self_attn.v_proj\n","model.layers.22.self_attn.o_proj\n","model.layers.22.self_attn.q_norm\n","model.layers.22.self_attn.k_norm\n","model.layers.22.mlp\n","model.layers.22.mlp.gate_proj\n","model.layers.22.mlp.up_proj\n","model.layers.22.mlp.down_proj\n","model.layers.22.mlp.act_fn\n","model.layers.22.input_layernorm\n","model.layers.22.post_attention_layernorm\n","model.layers.23\n","model.layers.23.self_attn\n","model.layers.23.self_attn.q_proj\n","model.layers.23.self_attn.k_proj\n","model.layers.23.self_attn.v_proj\n","model.layers.23.self_attn.o_proj\n","model.layers.23.self_attn.q_norm\n","model.layers.23.self_attn.k_norm\n","model.layers.23.mlp\n","model.layers.23.mlp.gate_proj\n","model.layers.23.mlp.up_proj\n","model.layers.23.mlp.down_proj\n","model.layers.23.mlp.act_fn\n","model.layers.23.input_layernorm\n","model.layers.23.post_attention_layernorm\n","model.layers.24\n","model.layers.24.self_attn\n","model.layers.24.self_attn.q_proj\n","model.layers.24.self_attn.k_proj\n","model.layers.24.self_attn.v_proj\n","model.layers.24.self_attn.o_proj\n","model.layers.24.self_attn.q_norm\n","model.layers.24.self_attn.k_norm\n","model.layers.24.mlp\n","model.layers.24.mlp.gate_proj\n","model.layers.24.mlp.up_proj\n","model.layers.24.mlp.down_proj\n","model.layers.24.mlp.act_fn\n","model.layers.24.input_layernorm\n","model.layers.24.post_attention_layernorm\n","model.layers.25\n","model.layers.25.self_attn\n","model.layers.25.self_attn.q_proj\n","model.layers.25.self_attn.k_proj\n","model.layers.25.self_attn.v_proj\n","model.layers.25.self_attn.o_proj\n","model.layers.25.self_attn.q_norm\n","model.layers.25.self_attn.k_norm\n","model.layers.25.mlp\n","model.layers.25.mlp.gate_proj\n","model.layers.25.mlp.up_proj\n","model.layers.25.mlp.down_proj\n","model.layers.25.mlp.act_fn\n","model.layers.25.input_layernorm\n","model.layers.25.post_attention_layernorm\n","model.layers.26\n","model.layers.26.self_attn\n","model.layers.26.self_attn.q_proj\n","model.layers.26.self_attn.k_proj\n","model.layers.26.self_attn.v_proj\n","model.layers.26.self_attn.o_proj\n","model.layers.26.self_attn.q_norm\n","model.layers.26.self_attn.k_norm\n","model.layers.26.mlp\n","model.layers.26.mlp.gate_proj\n","model.layers.26.mlp.up_proj\n","model.layers.26.mlp.down_proj\n","model.layers.26.mlp.act_fn\n","model.layers.26.input_layernorm\n","model.layers.26.post_attention_layernorm\n","model.layers.27\n","model.layers.27.self_attn\n","model.layers.27.self_attn.q_proj\n","model.layers.27.self_attn.k_proj\n","model.layers.27.self_attn.v_proj\n","model.layers.27.self_attn.o_proj\n","model.layers.27.self_attn.q_norm\n","model.layers.27.self_attn.k_norm\n","model.layers.27.mlp\n","model.layers.27.mlp.gate_proj\n","model.layers.27.mlp.up_proj\n","model.layers.27.mlp.down_proj\n","model.layers.27.mlp.act_fn\n","model.layers.27.input_layernorm\n","model.layers.27.post_attention_layernorm\n","model.layers.28\n","model.layers.28.self_attn\n","model.layers.28.self_attn.q_proj\n","model.layers.28.self_attn.k_proj\n","model.layers.28.self_attn.v_proj\n","model.layers.28.self_attn.o_proj\n","model.layers.28.self_attn.q_norm\n","model.layers.28.self_attn.k_norm\n","model.layers.28.mlp\n","model.layers.28.mlp.gate_proj\n","model.layers.28.mlp.up_proj\n","model.layers.28.mlp.down_proj\n","model.layers.28.mlp.act_fn\n","model.layers.28.input_layernorm\n","model.layers.28.post_attention_layernorm\n","model.layers.29\n","model.layers.29.self_attn\n","model.layers.29.self_attn.q_proj\n","model.layers.29.self_attn.k_proj\n","model.layers.29.self_attn.v_proj\n","model.layers.29.self_attn.o_proj\n","model.layers.29.self_attn.q_norm\n","model.layers.29.self_attn.k_norm\n","model.layers.29.mlp\n","model.layers.29.mlp.gate_proj\n","model.layers.29.mlp.up_proj\n","model.layers.29.mlp.down_proj\n","model.layers.29.mlp.act_fn\n","model.layers.29.input_layernorm\n","model.layers.29.post_attention_layernorm\n","model.layers.30\n","model.layers.30.self_attn\n","model.layers.30.self_attn.q_proj\n","model.layers.30.self_attn.k_proj\n","model.layers.30.self_attn.v_proj\n","model.layers.30.self_attn.o_proj\n","model.layers.30.self_attn.q_norm\n","model.layers.30.self_attn.k_norm\n","model.layers.30.mlp\n","model.layers.30.mlp.gate_proj\n","model.layers.30.mlp.up_proj\n","model.layers.30.mlp.down_proj\n","model.layers.30.mlp.act_fn\n","model.layers.30.input_layernorm\n","model.layers.30.post_attention_layernorm\n","model.layers.31\n","model.layers.31.self_attn\n","model.layers.31.self_attn.q_proj\n","model.layers.31.self_attn.k_proj\n","model.layers.31.self_attn.v_proj\n","model.layers.31.self_attn.o_proj\n","model.layers.31.self_attn.q_norm\n","model.layers.31.self_attn.k_norm\n","model.layers.31.mlp\n","model.layers.31.mlp.gate_proj\n","model.layers.31.mlp.up_proj\n","model.layers.31.mlp.down_proj\n","model.layers.31.mlp.act_fn\n","model.layers.31.input_layernorm\n","model.layers.31.post_attention_layernorm\n","model.layers.32\n","model.layers.32.self_attn\n","model.layers.32.self_attn.q_proj\n","model.layers.32.self_attn.k_proj\n","model.layers.32.self_attn.v_proj\n","model.layers.32.self_attn.o_proj\n","model.layers.32.self_attn.q_norm\n","model.layers.32.self_attn.k_norm\n","model.layers.32.mlp\n","model.layers.32.mlp.gate_proj\n","model.layers.32.mlp.up_proj\n","model.layers.32.mlp.down_proj\n","model.layers.32.mlp.act_fn\n","model.layers.32.input_layernorm\n","model.layers.32.post_attention_layernorm\n","model.layers.33\n","model.layers.33.self_attn\n","model.layers.33.self_attn.q_proj\n","model.layers.33.self_attn.k_proj\n","model.layers.33.self_attn.v_proj\n","model.layers.33.self_attn.o_proj\n","model.layers.33.self_attn.q_norm\n","model.layers.33.self_attn.k_norm\n","model.layers.33.mlp\n","model.layers.33.mlp.gate_proj\n","model.layers.33.mlp.up_proj\n","model.layers.33.mlp.down_proj\n","model.layers.33.mlp.act_fn\n","model.layers.33.input_layernorm\n","model.layers.33.post_attention_layernorm\n","model.layers.34\n","model.layers.34.self_attn\n","model.layers.34.self_attn.q_proj\n","model.layers.34.self_attn.k_proj\n","model.layers.34.self_attn.v_proj\n","model.layers.34.self_attn.o_proj\n","model.layers.34.self_attn.q_norm\n","model.layers.34.self_attn.k_norm\n","model.layers.34.mlp\n","model.layers.34.mlp.gate_proj\n","model.layers.34.mlp.up_proj\n","model.layers.34.mlp.down_proj\n","model.layers.34.mlp.act_fn\n","model.layers.34.input_layernorm\n","model.layers.34.post_attention_layernorm\n","model.layers.35\n","model.layers.35.self_attn\n","model.layers.35.self_attn.q_proj\n","model.layers.35.self_attn.k_proj\n","model.layers.35.self_attn.v_proj\n","model.layers.35.self_attn.o_proj\n","model.layers.35.self_attn.q_norm\n","model.layers.35.self_attn.k_norm\n","model.layers.35.mlp\n","model.layers.35.mlp.gate_proj\n","model.layers.35.mlp.up_proj\n","model.layers.35.mlp.down_proj\n","model.layers.35.mlp.act_fn\n","model.layers.35.input_layernorm\n","model.layers.35.post_attention_layernorm\n","model.norm\n","model.rotary_emb\n","lm_head\n"]}],"source":["for name, module in model.named_modules():\n","    print(name)"]},{"cell_type":"markdown","metadata":{"id":"dPzAKOOvJF-Q"},"source":["### 파인튜닝 진행 상황 확인 코드"]},{"cell_type":"code","execution_count":null,"metadata":{"collapsed":true,"jupyter":{"outputs_hidden":true},"id":"l2knLoQfJF-R","outputId":"b960f308-6024-4dbe-dbc1-98c3f532e4a6"},"outputs":[{"name":"stdout","output_type":"stream","text":["2.99184339314845\n","918\n","TrainerState(epoch=2.99184339314845, global_step=918, max_steps=918, logging_steps=10, eval_steps=500, save_steps=100, train_batch_size=1, num_train_epochs=3, num_input_tokens_seen=0, total_flos=3.137344340725801e+17, log_history=[{'loss': 2.7026, 'grad_norm': 0.41688981652259827, 'learning_rate': 1.4852941176470589e-05, 'epoch': 0.03262642740619902, 'step': 10}, {'loss': 2.6084, 'grad_norm': 0.5307020545005798, 'learning_rate': 1.4689542483660132e-05, 'epoch': 0.06525285481239804, 'step': 20}, {'loss': 2.5327, 'grad_norm': 0.6510335206985474, 'learning_rate': 1.4526143790849673e-05, 'epoch': 0.09787928221859707, 'step': 30}, {'loss': 2.4508, 'grad_norm': 0.41874781250953674, 'learning_rate': 1.4362745098039217e-05, 'epoch': 0.13050570962479607, 'step': 40}, {'loss': 2.4498, 'grad_norm': 0.3602053225040436, 'learning_rate': 1.4199346405228758e-05, 'epoch': 0.1631321370309951, 'step': 50}, {'loss': 2.4242, 'grad_norm': 0.3123202621936798, 'learning_rate': 1.4035947712418301e-05, 'epoch': 0.19575856443719414, 'step': 60}, {'loss': 2.3407, 'grad_norm': 0.25367507338523865, 'learning_rate': 1.3872549019607844e-05, 'epoch': 0.22838499184339314, 'step': 70}, {'loss': 2.2942, 'grad_norm': 0.2777913510799408, 'learning_rate': 1.3709150326797385e-05, 'epoch': 0.26101141924959215, 'step': 80}, {'loss': 2.2857, 'grad_norm': 0.24477900564670563, 'learning_rate': 1.3545751633986928e-05, 'epoch': 0.2936378466557912, 'step': 90}, {'loss': 2.2638, 'grad_norm': 0.24796263873577118, 'learning_rate': 1.3382352941176471e-05, 'epoch': 0.3262642740619902, 'step': 100}, {'loss': 2.2022, 'grad_norm': 0.2536380887031555, 'learning_rate': 1.3218954248366014e-05, 'epoch': 0.35889070146818924, 'step': 110}, {'loss': 2.1868, 'grad_norm': 0.2539331614971161, 'learning_rate': 1.3055555555555555e-05, 'epoch': 0.3915171288743883, 'step': 120}, {'loss': 2.1916, 'grad_norm': 0.28130465745925903, 'learning_rate': 1.28921568627451e-05, 'epoch': 0.42414355628058725, 'step': 130}, {'loss': 2.16, 'grad_norm': 0.30663174390792847, 'learning_rate': 1.2728758169934641e-05, 'epoch': 0.4567699836867863, 'step': 140}, {'loss': 2.1487, 'grad_norm': 0.3120843470096588, 'learning_rate': 1.2565359477124184e-05, 'epoch': 0.4893964110929853, 'step': 150}, {'loss': 2.178, 'grad_norm': 0.2649107277393341, 'learning_rate': 1.2401960784313725e-05, 'epoch': 0.5220228384991843, 'step': 160}, {'loss': 2.1086, 'grad_norm': 0.30136924982070923, 'learning_rate': 1.2238562091503268e-05, 'epoch': 0.5546492659053833, 'step': 170}, {'loss': 2.0804, 'grad_norm': 0.3256382346153259, 'learning_rate': 1.207516339869281e-05, 'epoch': 0.5872756933115824, 'step': 180}, {'loss': 2.087, 'grad_norm': 0.3505503535270691, 'learning_rate': 1.1911764705882352e-05, 'epoch': 0.6199021207177814, 'step': 190}, {'loss': 2.0724, 'grad_norm': 0.3409627676010132, 'learning_rate': 1.1748366013071896e-05, 'epoch': 0.6525285481239804, 'step': 200}, {'loss': 2.0466, 'grad_norm': 0.34013062715530396, 'learning_rate': 1.1584967320261438e-05, 'epoch': 0.6851549755301795, 'step': 210}, {'loss': 2.044, 'grad_norm': 0.3706781268119812, 'learning_rate': 1.142156862745098e-05, 'epoch': 0.7177814029363785, 'step': 220}, {'loss': 2.0467, 'grad_norm': 0.35287824273109436, 'learning_rate': 1.1258169934640523e-05, 'epoch': 0.7504078303425775, 'step': 230}, {'loss': 2.0428, 'grad_norm': 0.4210397005081177, 'learning_rate': 1.1094771241830066e-05, 'epoch': 0.7830342577487766, 'step': 240}, {'loss': 2.032, 'grad_norm': 0.3522937595844269, 'learning_rate': 1.0931372549019607e-05, 'epoch': 0.8156606851549756, 'step': 250}, {'loss': 2.0297, 'grad_norm': 0.38010373711586, 'learning_rate': 1.0767973856209152e-05, 'epoch': 0.8482871125611745, 'step': 260}, {'loss': 1.9862, 'grad_norm': 0.44359222054481506, 'learning_rate': 1.0604575163398693e-05, 'epoch': 0.8809135399673735, 'step': 270}, {'loss': 1.9958, 'grad_norm': 0.37781426310539246, 'learning_rate': 1.0441176470588234e-05, 'epoch': 0.9135399673735726, 'step': 280}, {'loss': 1.9958, 'grad_norm': 0.39921170473098755, 'learning_rate': 1.0277777777777779e-05, 'epoch': 0.9461663947797716, 'step': 290}, {'loss': 2.0158, 'grad_norm': 0.4065484404563904, 'learning_rate': 1.011437908496732e-05, 'epoch': 0.9787928221859706, 'step': 300}, {'loss': 1.9681, 'grad_norm': 0.5121746063232422, 'learning_rate': 9.950980392156863e-06, 'epoch': 1.0097879282218598, 'step': 310}, {'loss': 1.9851, 'grad_norm': 0.40321001410484314, 'learning_rate': 9.787581699346406e-06, 'epoch': 1.0424143556280587, 'step': 320}, {'loss': 1.9532, 'grad_norm': 0.4618838429450989, 'learning_rate': 9.624183006535949e-06, 'epoch': 1.0750407830342577, 'step': 330}, {'loss': 1.9773, 'grad_norm': 0.4916824698448181, 'learning_rate': 9.46078431372549e-06, 'epoch': 1.1076672104404568, 'step': 340}, {'loss': 1.9703, 'grad_norm': 0.482605904340744, 'learning_rate': 9.297385620915035e-06, 'epoch': 1.1402936378466557, 'step': 350}, {'loss': 1.9598, 'grad_norm': 0.3915671110153198, 'learning_rate': 9.133986928104576e-06, 'epoch': 1.1729200652528549, 'step': 360}, {'loss': 1.9561, 'grad_norm': 0.4927343726158142, 'learning_rate': 8.970588235294117e-06, 'epoch': 1.2055464926590538, 'step': 370}, {'loss': 1.9395, 'grad_norm': 0.4742579758167267, 'learning_rate': 8.80718954248366e-06, 'epoch': 1.238172920065253, 'step': 380}, {'loss': 1.9405, 'grad_norm': 0.5470145344734192, 'learning_rate': 8.643790849673203e-06, 'epoch': 1.2707993474714518, 'step': 390}, {'loss': 1.9149, 'grad_norm': 0.5191693902015686, 'learning_rate': 8.480392156862745e-06, 'epoch': 1.3034257748776508, 'step': 400}, {'loss': 1.9403, 'grad_norm': 0.5745352506637573, 'learning_rate': 8.316993464052287e-06, 'epoch': 1.33605220228385, 'step': 410}, {'loss': 1.9164, 'grad_norm': 0.5284703969955444, 'learning_rate': 8.153594771241831e-06, 'epoch': 1.368678629690049, 'step': 420}, {'loss': 1.9343, 'grad_norm': 0.5304123163223267, 'learning_rate': 7.990196078431372e-06, 'epoch': 1.401305057096248, 'step': 430}, {'loss': 1.9287, 'grad_norm': 0.5550795197486877, 'learning_rate': 7.826797385620915e-06, 'epoch': 1.433931484502447, 'step': 440}, {'loss': 1.9184, 'grad_norm': 0.5064936280250549, 'learning_rate': 7.663398692810458e-06, 'epoch': 1.466557911908646, 'step': 450}, {'loss': 1.926, 'grad_norm': 0.5166084170341492, 'learning_rate': 7.5e-06, 'epoch': 1.499184339314845, 'step': 460}, {'loss': 1.8971, 'grad_norm': 0.5952816605567932, 'learning_rate': 7.336601307189543e-06, 'epoch': 1.531810766721044, 'step': 470}, {'loss': 1.8932, 'grad_norm': 0.5309725999832153, 'learning_rate': 7.173202614379085e-06, 'epoch': 1.564437194127243, 'step': 480}, {'loss': 1.8544, 'grad_norm': 0.5137014389038086, 'learning_rate': 7.009803921568628e-06, 'epoch': 1.597063621533442, 'step': 490}, {'loss': 1.8731, 'grad_norm': 0.5619836449623108, 'learning_rate': 6.846405228758171e-06, 'epoch': 1.629690048939641, 'step': 500}, {'loss': 1.8471, 'grad_norm': 0.6024863123893738, 'learning_rate': 6.683006535947712e-06, 'epoch': 1.6623164763458402, 'step': 510}, {'loss': 1.9194, 'grad_norm': 0.5217866897583008, 'learning_rate': 6.519607843137255e-06, 'epoch': 1.6949429037520392, 'step': 520}, {'loss': 1.8984, 'grad_norm': 0.727905809879303, 'learning_rate': 6.356209150326797e-06, 'epoch': 1.727569331158238, 'step': 530}, {'loss': 1.8622, 'grad_norm': 0.6015773415565491, 'learning_rate': 6.19281045751634e-06, 'epoch': 1.7601957585644372, 'step': 540}, {'loss': 1.8758, 'grad_norm': 0.5993648171424866, 'learning_rate': 6.029411764705883e-06, 'epoch': 1.7928221859706364, 'step': 550}, {'loss': 1.8781, 'grad_norm': 0.6250214576721191, 'learning_rate': 5.866013071895425e-06, 'epoch': 1.8254486133768353, 'step': 560}, {'loss': 1.8437, 'grad_norm': 0.6518300175666809, 'learning_rate': 5.7026143790849676e-06, 'epoch': 1.8580750407830342, 'step': 570}, {'loss': 1.8889, 'grad_norm': 0.622342586517334, 'learning_rate': 5.5392156862745104e-06, 'epoch': 1.8907014681892331, 'step': 580}, {'loss': 1.8585, 'grad_norm': 0.7480093240737915, 'learning_rate': 5.3758169934640525e-06, 'epoch': 1.9233278955954323, 'step': 590}, {'loss': 1.8212, 'grad_norm': 0.6723009347915649, 'learning_rate': 5.212418300653595e-06, 'epoch': 1.9559543230016314, 'step': 600}, {'loss': 1.8483, 'grad_norm': 0.5779903531074524, 'learning_rate': 5.049019607843137e-06, 'epoch': 1.9885807504078303, 'step': 610}, {'loss': 1.839, 'grad_norm': 0.8057702779769897, 'learning_rate': 4.885620915032679e-06, 'epoch': 2.0195758564437196, 'step': 620}, {'loss': 1.8525, 'grad_norm': 0.6916432976722717, 'learning_rate': 4.722222222222222e-06, 'epoch': 2.0522022838499185, 'step': 630}, {'loss': 1.8538, 'grad_norm': 0.5958628058433533, 'learning_rate': 4.558823529411764e-06, 'epoch': 2.0848287112561175, 'step': 640}, {'loss': 1.8145, 'grad_norm': 0.6946627497673035, 'learning_rate': 4.395424836601307e-06, 'epoch': 2.1174551386623164, 'step': 650}, {'loss': 1.7736, 'grad_norm': 0.6510292291641235, 'learning_rate': 4.23202614379085e-06, 'epoch': 2.1500815660685153, 'step': 660}, {'loss': 1.8241, 'grad_norm': 0.7307106256484985, 'learning_rate': 4.068627450980392e-06, 'epoch': 2.1827079934747147, 'step': 670}, {'loss': 1.8556, 'grad_norm': 0.7658212184906006, 'learning_rate': 3.905228758169935e-06, 'epoch': 2.2153344208809136, 'step': 680}, {'loss': 1.8803, 'grad_norm': 0.6873112320899963, 'learning_rate': 3.741830065359477e-06, 'epoch': 2.2479608482871125, 'step': 690}, {'loss': 1.8097, 'grad_norm': 0.6221736073493958, 'learning_rate': 3.57843137254902e-06, 'epoch': 2.2805872756933114, 'step': 700}, {'loss': 1.8109, 'grad_norm': 0.7059026956558228, 'learning_rate': 3.4150326797385623e-06, 'epoch': 2.3132137030995104, 'step': 710}, {'loss': 1.8606, 'grad_norm': 0.5856063961982727, 'learning_rate': 3.2516339869281048e-06, 'epoch': 2.3458401305057097, 'step': 720}, {'loss': 1.7753, 'grad_norm': 0.6915057897567749, 'learning_rate': 3.088235294117647e-06, 'epoch': 2.3784665579119086, 'step': 730}, {'loss': 1.8108, 'grad_norm': 0.6558476686477661, 'learning_rate': 2.9248366013071897e-06, 'epoch': 2.4110929853181076, 'step': 740}, {'loss': 1.8269, 'grad_norm': 0.6850897073745728, 'learning_rate': 2.761437908496732e-06, 'epoch': 2.443719412724307, 'step': 750}, {'loss': 1.7882, 'grad_norm': 0.5892251133918762, 'learning_rate': 2.5980392156862746e-06, 'epoch': 2.476345840130506, 'step': 760}, {'loss': 1.8582, 'grad_norm': 0.6482437252998352, 'learning_rate': 2.434640522875817e-06, 'epoch': 2.5089722675367048, 'step': 770}, {'loss': 1.8497, 'grad_norm': 0.7033350467681885, 'learning_rate': 2.2712418300653595e-06, 'epoch': 2.5415986949429037, 'step': 780}, {'loss': 1.8008, 'grad_norm': 0.6216256022453308, 'learning_rate': 2.107843137254902e-06, 'epoch': 2.5742251223491026, 'step': 790}, {'loss': 1.8695, 'grad_norm': 0.7889624238014221, 'learning_rate': 1.9444444444444444e-06, 'epoch': 2.6068515497553015, 'step': 800}, {'loss': 1.8342, 'grad_norm': 0.7231360673904419, 'learning_rate': 1.781045751633987e-06, 'epoch': 2.639477977161501, 'step': 810}, {'loss': 1.8382, 'grad_norm': 0.6969519853591919, 'learning_rate': 1.6176470588235295e-06, 'epoch': 2.6721044045677, 'step': 820}, {'loss': 1.81, 'grad_norm': 0.7945506572723389, 'learning_rate': 1.454248366013072e-06, 'epoch': 2.7047308319738987, 'step': 830}, {'loss': 1.8283, 'grad_norm': 0.706940770149231, 'learning_rate': 1.2908496732026144e-06, 'epoch': 2.737357259380098, 'step': 840}, {'loss': 1.8016, 'grad_norm': 0.7331529259681702, 'learning_rate': 1.1274509803921568e-06, 'epoch': 2.769983686786297, 'step': 850}, {'loss': 1.7978, 'grad_norm': 0.7950146794319153, 'learning_rate': 9.640522875816995e-07, 'epoch': 2.802610114192496, 'step': 860}, {'loss': 1.7135, 'grad_norm': 0.7313573360443115, 'learning_rate': 8.006535947712418e-07, 'epoch': 2.835236541598695, 'step': 870}, {'loss': 1.7931, 'grad_norm': 0.6620938777923584, 'learning_rate': 6.372549019607844e-07, 'epoch': 2.867862969004894, 'step': 880}, {'loss': 1.7837, 'grad_norm': 0.7449600696563721, 'learning_rate': 4.7385620915032685e-07, 'epoch': 2.9004893964110927, 'step': 890}, {'loss': 1.843, 'grad_norm': 0.7492356300354004, 'learning_rate': 3.104575163398693e-07, 'epoch': 2.933115823817292, 'step': 900}, {'loss': 1.8015, 'grad_norm': 0.7678380608558655, 'learning_rate': 1.4705882352941178e-07, 'epoch': 2.965742251223491, 'step': 910}, {'train_runtime': 2638.9497, 'train_samples_per_second': 2.787, 'train_steps_per_second': 0.348, 'total_flos': 3.137344340725801e+17, 'train_loss': 1.9744175524493448, 'epoch': 2.99184339314845, 'step': 918}], best_metric=None, best_global_step=None, best_model_checkpoint=None, is_local_process_zero=True, is_world_process_zero=True, is_hyper_param_search=False, trial_name=None, trial_params=None, stateful_callbacks={'TrainerControl': {'args': {'should_training_stop': True, 'should_epoch_stop': False, 'should_save': True, 'should_evaluate': False, 'should_log': False}, 'attributes': {}}})\n"]}],"source":["print(trainer.state.epoch)\n","print(trainer.state.global_step)\n","print(trainer.state)\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"4emYabb7JF-S"},"outputs":[],"source":[]},{"cell_type":"code","execution_count":null,"metadata":{"id":"pN4XZ2EAJF-T"},"outputs":[],"source":[]}],"metadata":{"colab":{"provenance":[]},"kernelspec":{"display_name":"Python 3 (ipykernel)","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.10.12"}},"nbformat":4,"nbformat_minor":0}